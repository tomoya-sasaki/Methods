---
title: "Regresion and ATE"
author: "Tomoya Sasaki"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---
\newcommand{\E}{\mathbb{E}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ATE}{\textsf{ATE}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\boldeta}{\bm{\eta}}

\newcommand{\Yi}{Y_i}
\newcommand{\bXXi}{\bX_i}
\newcommand{\XXi}{X_i}
\newcommand{\bXj}{\bX_j}
\newcommand{\Yj}{Y_j}
\newcommand{\Dj}{D_j}
\newcommand{\Di}{D_i}
\newcommand{\bZj}{\bZ_j}
\newcommand{\bZi}{\bZ_i}
\newcommand{\Zi}{Z_i}

\newcommand{\xxi}{x_i}

\newcommand{\Yione}{Y_i(1)}
\newcommand{\Yizero}{Y_i(0)}
\newcommand{\Yid}{Y_i(d)}
\newcommand{\Yioo}{Y_i(1, 1)}
\newcommand{\Yioze}{Y_i(1, 0)}
\newcommand{\Yizeo}{Y_i(0, 1)}
\newcommand{\Yizeze}{Y_i(0, 0)}
\newcommand{\Yiod}{Y_i(1, d)}
\newcommand{\Yized}{Y_i(0, d)}
\newcommand{\Yizd}{Y_i(z, d)}
\newcommand{\Yizone}{Y_i(z, 1)}
\newcommand{\Yizzero}{Y_i(z, 0)}
\newcommand{\Done}{D(1)}
\newcommand{\Dzero}{D(0)}
\newcommand{\Dione}{D_i(1)}
\newcommand{\Dizero}{D_i(0)}
\newcommand{\Diz}{D_i(z)}
\newcommand{\Dizi}{D_i(\Zi)}
\newcommand{\YioDione}{Y_i(1, \Dione)}
\newcommand{\YizeDione}{Y_i(0, \Dione)}
\newcommand{\YioDizero}{Y_i(1, \Dizero)}
\newcommand{\YizeDizero}{Y_i(0, \Dizero)}
\newcommand{\Yizdz}{Y_i(\Zi, \Dizi)}
\newcommand{\Yizi}{Y_i(\Zi)}
\newcommand{\Yiz}{Y_i(z)}
\newcommand{\nsumi}{\sum_{i = 1}^n}
\newcommand{\Yist}{\Yi^*}
\newcommand{\Dist}{\Di^*}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, #cache = TRUE, 
                      fig.align = "center", fig.width = 7, fig.height = 4)
```

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
```


<!-- Start here -->
# Binary treatment, no control variables
## Constant additive unit causal effect
* The assumption: the causal effect is the same across all the units
* $\beta = \E[\Yione] - \E[\Yizero]$ for $\forall i$
* Data generating process
\begin{align}
\Yi = \alpha + \beta \Di + \epsilon_i
\end{align}
* Treatment variable $\Di$, $\E[\Yizero] = \alpha$, and $\E[\epsilon_i] = 0$
* Note that we have $\alpha$ so $\E[\epsilon_i] = 0$ is not really an assumption, but by construction

### Simulation Setting
```{r}
N <- 1000
M <- 1000
```


### Generate data
```{r}
set.seed(123)
b <- 2 # treatment effect (constant across all units)
e <- rnorm(mean = 0, sd = 4, n = N)
# potential outcomes
y1 <- b + e
y0 <- e
# treatment asignment
x <- sample(0:1, prob = c(1, 1), size = N, replace = TRUE)
# observed outcomes
y <- ifelse(x == 1, y1, y0)
```

### Estimate via regression
```{r}
mod1 <- lm(y ~ x)
coefficients(summary(mod1))
```

### Estimate via difference in means
```{r}
mean(y[x == 1]) - mean(y[x == 0])
```

### Regression VS difference in means
* In the simulation, I only change the treatment assignment
* Results of two estimators are identical
* A regression estimator is a unbiased estimator of the difference in means estimator 
as long as the treatment varaiable is binary

```{r}
# create function
sim_reg_diff_mean <- function(.y1, .y0) {
  # random assignment of treatment
  x <- sample(0:1, prob = c(1, 1), size = N, replace = TRUE)
  # observed y
  y <- ifelse(x == 1, .y1, .y0)
  # regression
  mod <- lm(y ~ x)
  # diff-in-means
  dif <- mean(y[x == 1]) - mean(y[x == 0])
  return( c(mod$coef[2], dif) )
}

set.seed(123)
# simulation 
# use replicate and generated data above
res <- replicate(n = M, sim_reg_diff_mean(y1, y0))
# two estimators are the same
all.equal(res[1, ], res[2, ])
```

### Plot results
```{r}
xup <- 3
xdown <- 1
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 0.5), labels = seq(xdown, xup, by = 0.5))
  abline(v = 2, col = "red")
  title(main = "Normal OLS: an unbiased estimator of ATE")
}
```
### Calculate RMSE
```{r}
rmse1 <- apply(res, 1, function(x) sqrt(mean((x - 2)^2)))
```


## Treatment effect heterogeneity
* Allow for heterogeneous treatment effects: each unit has different causal effects
* $\beta + \epsilon_i(1) - \epsilon_i(0) = \E[\Yione] - \E[\Yizero]$ for $\forall i$
* $\beta$ is ATE
* Data generating process
\begin{align}
\Yi = \alpha + \beta \Di + \epsilon_i(\Di)
\end{align}
* Treatment variable $\Di$, $\E[\Yizero] = \alpha$, and $\E[\epsilon_i(1)] = \E[\epsilon_i(0)] = 0$
* Relax the assumption of $\epsilon_i = \epsilon_i(1) = \epsilon_i(0)$
* The example below shows that the OLS estimator that does not assume treatment effect heterogeneity
is an unbiasesd estimator of the ATE (that allows for treatment effect heterogeneity). 

### Generate data
```{r}
set.seed(123)
# this time with different error term structure
e1 <- rnorm(mean = 0, sd = 4, n = N)
e0 <- rnorm(mean = 0, sd = 4, n = N)
# potential outcomes
y1 <- b + e1
y0 <- e0
# treatment asignment
x <- sample(0:1, prob = c(1, 1), size = N, replace = TRUE)
# observed outcomes
y <- ifelse(x == 1, y1, y0)
```

### Estimate via difference in means
```{r}
mean(y[x == 1]) - mean(y[x == 0])
```

### Estimate via regression
```{r}
mod1 <- lm(y ~ x)
coefficients(summary(mod1))
```

### Regression VS difference in means under heterogenous traetment effects
* Results of two estimators are identical
* Again, a regression estimator is a unbiased estimator of the difference in means estimator 
as long as the treatment varaiable is binary
```{r}
set.seed(123)
# simulation 
# use replicate and generated data
res <- replicate(n = M, sim_reg_diff_mean(y1, y0))
# two estimators are the same
all.equal(res[1, ], res[2, ])
```

### Plot results
* Note that this time true ATE is not 2 because of heterogeneity
```{r}
xup <- 3
xdown <- 1
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 0.5), labels = seq(xdown, xup, by = 0.5))
  abline(v = mean(y1 - y0), col = "red")
  title(main = "Normal OLS: an unbiased estimator of ATE")
}
```

### Calculate RMSE
```{r}
rmse2 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2)))
```


# Binary treatment, with control variables
## Heterogeneous treatment effect
* If you assume constant treatment effect, you can just regression with control variables
* However, if you assume heterogeneous treatment effect, you need to use a saturated model:
\begin{align}
\Yi = \sum_x B_{xi} \gamma_x + \beta \Di + \epsilon_i
\end{align}
where $B_{xi}$ is a dummy variable for unique combination of $x_i$.
* We compare results from a saturated model with results from a normal OLS:
\begin{align}
\Yi = \gamma^\top x_i + \beta \Di + e_i
\end{align}
* This model is linear in covariates by construction whereas the linear assumption does not necessarily holds for 
a normal OLS


### Generate data
```{r}
set.seed(123)
b1 <- 2
# control variables
lenx2 <- 5 # 1 to 5
lenx3 <- 4 # 1 to 3
lenx4 <- 3 # 1 to 4
lenx5 <- 4 # 1 to 4
# probability of covariates
probx2 <- sample(1:5, size = lenx2, replace = TRUE)
probx3 <- sample(1:5, size = lenx3, replace = TRUE)
probx4 <- sample(1:5, size = lenx4, replace = TRUE)
probx5 <- sample(1:5, size = lenx5, replace = TRUE)
# generate covariates
x2 <- sample(1:lenx2, prob = probx2, size = N, replace = TRUE)
x3 <- sample(1:lenx3, prob = probx3, size = N, replace = TRUE)
x4 <- sample(1:lenx4, prob = probx4, size = N, replace = TRUE)
x5 <- sample(1:lenx5, prob = probx5, size = N, replace = TRUE)
raw_mat <- cbind(x2, x3)
# different error term structure
e1 <- rnorm(mean = 0, sd = 4, n = N)
e0 <- rnorm(mean = 0, sd = 4, n = N)
```

### Prepare to run saturated model: create dummy variables
```{r}
# possible combination of control variables
comb <- expand.grid(1:lenx2, 1:lenx3)#, 1:lenx4)#, 1:lenx5)
colnames(comb) <- c("x2", "x3")#, "x4")#, "x5")
# create unique ID for each combination
comb <- cbind(comb, id = 1:nrow(comb))
# control variables for each observation is mapped to IDs
comb_join <- inner_join(data.frame(x2 = x2, x3 = x3), data.frame(comb), 
                        by = c("x2" = "x2", "x3" = "x3"))
# create a matrix of dummy variables
matXs <- matrix(0, nrow = N, ncol = nrow(comb))

for (i in 1:nrow(comb_join)){
	.id <- comb_join$id[i]
	matXs[i, .id] <- 1
}
```

### Pattern 1
* Potential outcomes: linear
```{r}
# potential outcomes
y1 <- e1 + b1 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5
y0 <- e0 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5

# random assignment of treatment
# p <- apply(raw_mat, 1, sum)/max(apply(raw_mat, 1, sum) + 1)
p <- 1 / (1 + exp(raw_mat[, 1] - 0.5 * raw_mat[, 2] + 0.25))
x <- as.numeric(runif(length(p)) < p)
df <- data.frame(x = x, matXs)
# observed y
y <- ifelse(x == 1, y1, y0)
# regression
mod <- lm(y ~ . -1, df); coef(mod)[1]
# normal OLS
df <- data.frame(x = x, raw_mat)
mod2 <- lm(y ~ ., df); coef(mod2)[2]
mean(y1 - y0)
```

```{r}
# create function
sim_reg_saturated <- function(.y1, .y0, .mat, .N, .raw) {
  # random assignment of treatment
  # .p <- apply(.raw, 1, sum)/max(apply(.raw, 1, sum) + 1)
  .p <- 1 / (1 + exp(.raw[, 1] - 0.5 * .raw[, 2] + 0.25))
  x <- as.numeric(runif(length(.p)) < .p)
  df <- data.frame(x = x, .mat)
  # observed y
  y <- ifelse(x == 1, .y1, .y0)
  # satruated regression
  mod <- lm(y ~ . -1, df)

  # normal regression
  df2 <- data.frame(x = x, .raw)
  mod2 <- lm(y ~ ., df2)

  return( c(mod$coef[1], mod2$coef[2]) )
}

set.seed(123)
# simulation
# use replicate and generated data above
res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat))
```

* Plot results
* Note that this time true ATE is not 2 because of heterogeneity
```{r}
xup <- 3.5
xdown <- 1
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown , xup), xaxt = "n")
  axis(1, at = seq(xdown , xup, by = 0.5), labels = seq(xdown , xup, by = 0.5))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[1, ]), col = "blue")
  title(main = "Saturated regresssion")
}
```

```{r}
{
  hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown , xup), xaxt = "n")
  axis(1, at = seq(xdown , xup, by = 0.5), labels = seq(xdown , xup, by = 0.5))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[2, ]), col = "blue")
  title(main = "Normal OLS")
}
```

* Calculate RMSE

```{r}
rmse3 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2)))
```


### Generate data 2
* Potential outcomes: nonlinear
```{r}
# potential outcomes
y1 <- e1 + b1 + 3 * x2^2 + -2 * 1/x3# + b4 * x4 # + b5 * x5
y0 <- e0 + 3 * x2^2 -2 * 1/x3# + b4 * x4 # + b5 * x5

set.seed(123)
# simulation
# use replicate and generated data above
res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat))
```

* Plot results

```{r}
xup <- 7
xdown <- 1
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 1), labels = seq(xdown, xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[1, ]), col = "blue")
  title(main = "Saturated regresssion")
}
```

```{r}
{
  hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown , xup), xaxt = "n")
  axis(1, at = seq(xdown , xup, by = 1), labels = seq(xdown , xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[2, ]), col = "blue")
  title(main = "Normal OLS")
}
```

* Calculate RMSE

```{r}
rmse4 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2)))
```

### Constant effect revisited
* Potential outcomes: nonlinear
* But treatment effect is constant
```{r}
# potential outcomes
set.seed(123)
e <- rnorm(mean = 0, sd = 4, n = N)
y1 <- e + b1 + 3 * x2^2 + -2 * 1/x3# + b4 * x4 # + b5 * x5
y0 <- e + 3 * x2^2 + -2 * 1/x3# + b4 * x4 # + b5 * x5

set.seed(123)
# simulation
# use replicate and generated data above
res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat))
```

* Plot results
```{r}
xup <- 7
xdown <- 0
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 1), labels = seq(xdown, xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[1, ]), col = "blue")
  title(main = "Saturated regresssion")
}
{
  hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 1), labels = seq(xdown , xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[2, ]), col = "blue")
  title(main = "Normal OLS")
}
```

* Calculate RMSE

```{r}
rmse7 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2)))
```


## Saturated model when N is small
* Note that a saturatede model has more predictors than a model with the same set of control variables so
the performance could be worse when the sample size is small
* The example below shows the same simulation with different number of units
```{r}
N <- 100

set.seed(123)
b1 <- 2
# control variables
lenx2 <- 5 # 1 to 5
lenx3 <- 4 # 1 to 3
lenx4 <- 3 # 1 to 4
lenx5 <- 3 # 1 to 4
# probability of covariates
probx2 <- sample(1:5, size = lenx2, replace = TRUE)
probx3 <- sample(1:5, size = lenx3, replace = TRUE)
probx4 <- sample(1:5, size = lenx4, replace = TRUE)
probx5 <- sample(1:5, size = lenx5, replace = TRUE)
# generate covariates
x2 <- sample(1:lenx2, prob = probx2, size = N, replace = TRUE)
x3 <- sample(1:lenx3, prob = probx3, size = N, replace = TRUE)
x4 <- sample(1:lenx4, prob = probx4, size = N, replace = TRUE)
x5 <- sample(1:lenx5, prob = probx5, size = N, replace = TRUE)
raw_mat <- cbind(x2, x3)
# different error term structure
e1 <- rnorm(mean = 0, sd = 4, n = N)
e0 <- rnorm(mean = 0, sd = 4, n = N)
```

### Prepare to run saturated model: create dummy variables
```{r}
# possible combination of control variables
comb <- expand.grid(1:lenx2, 1:lenx3)#, 1:lenx4)#, 1:lenx5)
colnames(comb) <- c("x2", "x3")#, "x4")#, "x5")
# create unique ID for each combination
comb <- cbind(comb, id = 1:nrow(comb))
# control variables for each observation is mapped to IDs
comb_join <- inner_join(data.frame(x2 = x2, x3 = x3), data.frame(comb), 
                        by = c("x2" = "x2", "x3" = "x3"))
# create a matrix of dummy variables
matXs <- matrix(0, nrow = N, ncol = nrow(comb))

for (i in 1:nrow(comb_join)){
	.id <- comb_join$id[i]
	matXs[i, .id] <- 1
}
```

* Probability of receiving treatment: linear
* Potential outcomes: linear
```{r}
# create function
sim_reg_saturated <- function(.y1, .y0, .mat, .N, .raw) {
  # random assignment of treatment
  .p <- apply(.raw, 1, sum)/max(apply(.raw, 1, sum) + 1)
  x <- as.numeric(runif(length(.p)) < .p)
  df <- data.frame(x = x, .mat)
  # observed y
  y <- ifelse(x == 1, .y1, .y0)
  # satruated regression
  mod <- lm(y ~ . -1, df)

  # normal regression
  df2 <- data.frame(x = x, .raw)
  mod2 <- lm(y ~ ., df2)

  return( c(mod$coef[1], mod2$coef[2]) )
}

# potential outcomes
y1 <- e1 + b1 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5
y0 <- e0 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5

set.seed(123)
# simulation
# use replicate and generated data above
res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat))
```

* Plot results
* Note that this time true ATE is not 2 because of heterogeneity
```{r}
xup <- 5
xdown <- 0
{
  hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown , xup), xaxt = "n")
  axis(1, at = seq(xdown , xup, by = 1), labels = seq(xdown , xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[1, ]), col = "blue")
  title(main = "Saturated regresssion")
}
```

```{r}
{
  hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n")
  axis(1, at = seq(xdown, xup, by = 1), labels = seq(xdown , xup, by = 1))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  abline(v = mean(res[2, ]), col = "blue")
  title(main = "Normal OLS")
}
```

* Calculate RMSE

```{r}
rmse8 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2)))
```


# Compare RMSE for models with multiplc control variables
```{r}
rmses <- rbind(rmse3, rmse4, rmse7, rmse8)
rmses <- data.frame(c("linear", "nonlinear", "nonlinear", "linear"), rmses)
rownames(rmses) <- c("Pattern 1", "Pattern 2", "Constant Effect", "Small N")
colnames(rmses) <- c("Potential Outcomes", "Saturated Model", "Normal OLS")
knitr::kable(rmses)
```


<!-- ### Generate data 3 -->
<!-- * Probability of receiving treatment: nonlinear -->
<!-- * Potential outcomes: linear -->
<!-- ```{r} -->
<!-- # potential outcomes -->
<!-- y1 <- e1 + b1 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5 -->
<!-- y0 <- e0 + 3 * x2 + -2 * x3# + b4 * x4 # + b5 * x5 -->

<!-- # create function -->
<!-- sim_reg_saturated <- function(.y1, .y0, .mat, .N, .raw) { -->
<!--   # random assignment of treatment -->
<!--   # .p <- apply(.raw, 1, sum)^2 / max(apply(.raw, 1, sum) + 1) -->
<!--   .p <- 1 / (1 + exp(.raw[, 1] - 0.5 * .raw[, 2] + 0.25)) -->
<!--   x <- as.numeric(runif(length(.p)) < .p) -->
<!--   df <- data.frame(x = x, .mat) -->
<!--   # observed y -->
<!--   y <- ifelse(x == 1, .y1, .y0) -->
<!--   # satruated regression -->
<!--   mod <- lm(y ~ . -1, df) -->

<!--   # normal regression -->
<!--   df2 <- data.frame(x = x, .raw) -->
<!--   mod2 <- lm(y ~ ., df2) -->

<!--   return( c(mod$coef[1], mod2$coef[2]) ) -->
<!-- } -->

<!-- set.seed(123) -->
<!-- # simulation -->
<!-- # use replicate and generated data above -->
<!-- res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat)) -->
<!-- ``` -->

<!-- * Plot results -->

<!-- ```{r} -->
<!-- xup <- 3.5 -->
<!-- xdown <- 0.5 -->
<!-- { -->
<!--   hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n") -->
<!--   axis(1, at = seq(xdown , xup, by = .5), labels = seq(xdown , xup, by = .5)) -->
<!--   abline(v = mean(y1 - y0), col = "red") # true ATE is not 2 -->
<!--   abline(v = mean(res[1, ]), col = "blue") -->
<!--   title(main = "Saturated regresssion") -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- { -->
<!--   hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n") -->
<!--   axis(1, at = seq(xdown , xup, by = .5), labels = seq(xdown , xup, by = .5)) -->
<!--   abline(v = mean(y1 - y0), col = "red") # true ATE is not 2 -->
<!--   abline(v = mean(res[2, ]), col = "blue") -->
<!--   title(main = "Normal OLS") -->
<!-- } -->
<!-- ``` -->

<!-- * Calculate RMSE -->

<!-- ```{r} -->
<!-- rmse5 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2))) -->
<!-- ``` -->

<!-- ### Generate data 4 -->
<!-- * The same result as Generate data 3 -->
<!-- * Probability of receiving treatment: nonlinear -->
<!-- * Potential outcomes: nonlinear -->
<!-- ```{r} -->
<!-- # potential outcomes -->
<!-- y1 <- e1 + b1 + 3 * x2^2 + -2 * 1/x3# + b4 * x4 # + b5 * x5 -->
<!-- y0 <- e0 + 3 * x2^2 + -2 * 1/x3# + b4 * x4 # + b5 * x5 -->

<!-- set.seed(123) -->
<!-- # simulation -->
<!-- # use replicate and generated data above -->
<!-- res <- replicate(n = M, sim_reg_saturated(y1, y0, matXs, N, raw_mat)) -->
<!-- xup <- 7 -->
<!-- xdown <- 0 -->
<!-- { -->
<!--   hist(res[1, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n") -->
<!--   axis(1, at = seq(xdown , xup, by = 1), labels = seq(xdown , xup, by = 1)) -->
<!--   abline(v = mean(y1 - y0), col = "red") # true ATE is not 2 -->
<!--   abline(v = mean(res[1, ]), col = "blue") -->
<!--   title(main = "Saturated regresssion") -->
<!-- } -->
<!-- { -->
<!--   hist(res[2, ], breaks = 100, xlab = "Value", main = "", xlim = c(xdown, xup), xaxt = "n") -->
<!--   axis(1, at = seq(xdown , xup, by = 1), labels = seq(xdown , xup, by = 1)) -->
<!--   abline(v = mean(y1 - y0), col = "red") # true ATE is not 2 -->
<!--   abline(v = mean(res[2, ]), col = "blue") -->
<!--   title(main = "Normal OLS") -->
<!-- } -->
<!-- ``` -->

<!-- * Calculate RMSE -->

<!-- ```{r} -->
<!-- rmse6 <- apply(res, 1, function(x) sqrt(mean((x - mean(y1 - y0))^2))) -->
<!-- ``` -->

<!-- ### When control variables do not correlate with the treatment -->
<!-- * Comparing a normal regression VS a saturated model -->
```{r, eval = FALSE, include = FALSE}
sim_reg_compare <- function(.y1, .y0, .mat, .N, .raw) {
  # random assignment of treatment
  x <- sample(0:1, prob = c(1, 1), size = .N, replace = TRUE)
  df <- data.frame(x = x, .mat)
  # observed y
  y <- ifelse(x == 1, .y1, .y0)
  # regression
  mod <- lm(y ~ . -1, df)
  # naive regression
  df <- data.frame(x = x, .raw)
  mod2 <- lm(y ~ ., df)
  return( c(mod$coef[1], mod2$coef[2]) )
}

set.seed(123)
# simulation 
# use replicate and generated data above
res <- replicate(n = M, sim_reg_compare(y1, y0, matXs, N, cbind(x2, x3)))
```


```{r, include = FALSE}
t_col <- function(color, percent = 50, name = NULL) {
  #      color = color name
  #    percent = % transparency
  #       name = an optional name for the color

  ## Get RGB values for named color
  rgb.val <- col2rgb(color)

  ## Make new color using input color as base and alpha set by transparency
  t.col <- rgb(rgb.val[1], rgb.val[2], rgb.val[3],
               max = 255,
               alpha = (100 - percent) * 255 / 100,
               names = name)

  ## Save the color
  invisible(t.col)
}
```


```{r, include = FALSE, eval = FALSE}
{
  hist(res[1, ], freq = FALSE, xlab = "Value", main = "", breaks = seq(1.5, 5.5, by = 0.05), 
       xlim = c(1.5, 5.5), xaxt = "n", col = "lightblue", ylim = c(0, 4.5))
  hist(res[2, ], freq = FALSE, xlab = "", main = "", breaks = seq(1.5, 5.5, by = 0.05), 
       xlim = c(1.5, 5.5), xaxt = "n", ylab = "", col = t_col("lightpink", 70), add = TRUE)
  axis(1, at = seq(1.5, 5.5, by = 0.5), labels = seq(1.5, 5.5, by = 0.5))
  abline(v = mean(y1 - y0), col = "red") # true ATE is not 2
  title(main = "Compare Satruated VS Normal regressions")
}
```

<!-- ### Check consistency -->
```{r, include = FALSE, eval = FALSE}
gen_data <- function(ObsN) {
  N <- ObsN
  a <- 1
  b1 <- 2
  b2 <- -2 
  b3 <- 3
  # control variables
  x2 <- sample(1:3, prob = c(1, 1, 1), size = N, replace = TRUE)
  x3 <- sample(1:2, prob = c(1, 1), size = N, replace = TRUE)
  # different error term structure
  e1 <- rnorm(mean = 0, sd = 2, n = N)
  e0 <- rnorm(mean = 0, sd = 2, n = N)
  # potential outcomes
  y1 <- a + b1 + b2 * x2 + b3 * x3 + e1
  y0 <- a + b2 * x2 + b3 * x3 + e0
  return(list(data = data.frame(y1 = y1, y0 = y0, x2 = x2, x3 = x3), ATE = mean(y1 - y0))  )
}

sim_consistency <- function(ObsN) {
  d <- gen_data(ObsN)
  res <- sim_reg_saturated(.y1 = d$data$y1, .y0 = d$data$y0,
                           .mat = cbind(d$data$x2, d$data$x3), .N = ObsN)
  return(list(res = c(res, ATE = d$ATE), data = d) )
}

set.seed(123)
N <- c(1000, 2000, 5000, 10000, 100000, 1000000)

res_consitency <- sapply(N, sim_consistency)
```

