---
title: "Quant 1-1"
author: "Tomoya Sasaki"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
fontsize: 12pt
---
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\cR}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\muhat}{\hat{\mu}}
\newcommand{\thetahat}{\hat{\theta}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

# Probability Theory (2, 3, 4)
## Three axioms
* nonnegativity
* normalization
* additivity 

$\leadsto$  Check these when you see probability distributions

## Key concepts
### Conditional probability
\begin{align*}
P(A | B) = \frac{P(A \cap B)}{P(B)}
\end{align*}

### Law of total probability
* Let $A_1, \dots, A_n$ be disjoint events that form a partition of the sample space and assume P(A_i) > 0 for all $i$
\begin{align*}
P(B) &= P(A_1 \cap B) + P(A_2 \cap B) + \dots + P(A_n \cap B) \\
&= P(B | A_1) P(A_1) + \dots + P(B | A_n) P(A_n) \\
&= \sum_{i = 1}^n P(B | A_i) P(A_i) \\
&= P(B | A) P(A) + P(B | A^C) P(A^C)
\end{align*}
* Example: quiz 1-extra

### Bayes rule 
* Use the conditional probability law and the law of total probability
\begin{align*}
P(A | B) &= \frac{P(A \cap B)}{P(B)} \\
&= \frac{P(A) P(B | A)}{P(B)} \\
&= \frac{P(A) P(B | A)}{\sum_{i = 1}^n P(B | A_i) P(A_i)} \\
&= \frac{P(A) P(B | A)}{P(B | A) P(A) + P(B | A^C) P(A^C)}
\end{align*}
* Example: slide 2-23, quiz 1-3

### Independence
* Intuition: Events A and B are independent if knowing whether A occurred provides no information about whether B occurred 
* rolling two fair dice, getting one for one die and getting six for the other one
\begin{align*}
P(A \cap B) = P(A)P(B) \Leftrightarrow A \indep B
\end{align*}
* Implication
\begin{align*}
P(A) &= P(A | B) = P(A | B^C) \\
P(A \cap B) &= P(A)P(B) = P(A | B) P(B) = P(A | B^C) P(B^C)
\end{align*}
* Analogous concept: conditional independence 
* Example: slide 1-26, quiz 1A

## Random variables and distributions
### Random variables
* Definition: A random variable is a real valued function that maps the sample space ($\Omega$) to the real numbers ($\cR$)

### Probability law
* Definition: The probability law $P_X(x)$ of a random variable $X$ is a real-valued function that assigns probabilities to each of its possible values $x \in X$. i.e.) $P_X: x \rightarrow [0, 1]$. It encodes our knowledge or belief about the likelihood of the outcomes in $\Omega$.
* If random variables are discrete, we call it the probability mass function (PMF)
* If random variables are continuous, we call it the probability density function (PDF)

### Cumulative function
* A cumulative distribution function $f_Y(y)$ of a random variable $Y$ is a non-decreasing function that gives you the probability that 
$Y \leq y$: $F_Y(y) = Pr(Y \leq y)$
* Discrete: $F_Y(y) = \sum_{\forall Y^\prime \leq y} P_Y (Y^\prime)$
* Continuous: $F_Y(y) = \int_{- \infty}^{y} f_Y (Y^\prime) dY^\prime$ where $f_Y(y)$ is the PDF for a random variable $Y$

## Expectation and variance
* Summary of probability distribution
* Expectation (average): measure of location
\begin{align*}
& \text{Discrete: } \E[X] = \sum_x x P_X (x) \\
& \text{Continuous: } \E[X] = \int_{-\infty}^x f_X (x) dx
\end{align*}
* Variance: measure of dispersion
\begin{align*}
\V(X) = \E[(X - \E[X])^2] = \E[X^2 - 2 \E[X] X + \E[X]^2 ] = \E[X^2] - \E[X]^2
\end{align*}
* Examples: quiz 1-1B

## Multiple random variables
### Joint PMF and marginal PMF
* $f_{Y|X} (y|x)$ : conditional PDF of $Y$ given $X= x$
* $f_{Y} (y)$ : conditional PDF of $Y$
* Note $\int_x \int_y f_{X, Y} (x, y) dy dx = \int_y \int_x f_{Y|X} (y|x) f_X(x) dx dy = 1$

### Conditional expectation
* Discrete: $\E[Y|X = x] = \sum_y y Pr(Y = y | X = x) = \sum_y y Pr_{Y|X}(y | x)$
* Continuous: $\E[Y|X = x] = \int_{-\infty}^{\infty} y f_{Y|X}(y | x) dy$
* Note that $\E[Y | X = x]$ is a function of $X$ (i.e. $\E[Y | X = x]$ depends on the value of $X$)
* Law of total expectation: $\E_Y[Y] = \E_X[\E_Y[Y | X]]$
* Examples: quiz 1-1E

### Conditional variance
* Discrete: $\V(Y | X = x) = \sum_y (y - E[Y | X = x]) P_{Y | X} (y | x)$
* Continous: $\V(Y | X = x) = \int_{-\infty}^{\infty} (y - E[Y | X = x]) P_{Y | X} (y | x) dy$
* Law of total variance: $\V(Y) = \E[V(Y|X)] + \V(\E[Y|X])$

### Independence and covariance
* Independence imlies covariance (and hence correlation) equals zero but not the other way around
* Useful pictures on slide 4-31 and proof on 4-28
* Usage of conditional independence: multiple regression

# Hypothesis testing (5)
## Intuition
* The goal is to estimate the parameters of the population distribution (estimands). However, we only observe a part of random variables that are (supposed to be) generated by the population distribution (sample). We try to come up with functions of sample data (statistics) which we use to make an inference about the estimands (estimators). Estimators will give you particular values that are realized from a given sample (estimates).
* These are our assumptions about how the world works

## Properties of estimators
* There are multiple estimators you can choose from. 
* Researchers often rely on the distribution of an estimator due to repeated sampling (the sampling distribution)
* Nice summary on slide 5-1-36, examples on the slide 1-2 and 1-4

### Finite-sample properties
* Unbiasedness: Is the sampling distribution of our estimator centered at the true parameter value? $\E[\thetahat^*] = \mu$ if and only if bias is zero
* Efficiency: Is the variance of the sampling distribution of our estimator reasonably small? $\V(\thetahat^*) \leq \V(\thetahat)$

### Asymptotioc properties ($n \rightarrow \infty$)
* Consistency: As our sample size grows to infinity, does the sampling distribution of our estimator converge to the true parameter value? $\thetahat^* \rightarrow \theta$ (Law of large numbers)
* Asymptotioc normality: As our sample size grows large, does the sampling distribution of our estimator approach a normal distribution? $\sqrt{n}(\thetahat^* - \theta) \rightarrow \cN(0, \sigma^2)$ (Central limit theorem)

### Examples of estimators with properties above
* OLS: all of these
* MLE: all but unbiasedness

### Mean squared error
* Examine and bias and variance of the estimator
\begin{align*}
MSE(\thetahat) = \E[ (\thetahat - \theta)^2  ] = (\E[\thetahat] - \theta) + \V(\thetahat) + \text{Bias}(\thetahat) + \text{Variance} (\thetahat)
\end{align*}

## Confidence interval
* The sampling distribution of the sample average is $\bar{X}_n \sim \cN(\mu, \sigma^2/n)$
* Note that $\mu$ and $\sigma$ are population parameters and $\bar{X}_n$ is an estimate
* Useful summary on the slide 5-2-22
* Examples: quiz 1-1D, 2-2

### Population is normal with know variance ($\sigma^2$)
* Derive a 95% confidence interval of a parameter $\mu$
\begin{align*}
&Pr(\bar{X}_n - 1.96 \sigma/\sqrt{n} < \mu < \bar{X}_n + 1.96 \sigma/\sqrt{n}) = .95 \\
&\therefore [\bar{X}_n - 1.96 \sigma/\sqrt{n}, \bar{X}_n + 1.96 \sigma/\sqrt{n}] 
\end{align*}
* More general form: Let $Z_{\alpha/2}$ be the $100(1/\alpha/2)$ percentile of the standard normal distribution
\begin{align*}
[\bar{X}_n - Z_{\alpha/2} \sigma / \sqrt{n}, \bar{X}_n + Z_{\alpha/2} \sigma / \sqrt{n}] 
\end{align*}

### Population is normal with unknow variance
* Use sample variance and the t-distribution
* Sample variance: $S_n^2 = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X}_n)^2$ (unbiased and consistent)
* t-distribution with $n-1$ degree of freedom: $\tau_{n-1} \sim \frac{\bar{X}_n - \mu}{S_n / \sqrt{n}}$
* To construct a confidence interval, swap $Z$ with $t$ which comes from t-distribution and $\sigma$ with $S_n$
\begin{align*}
[\bar{X}_n - t_{\alpha/2} S_n / \sqrt{n}, \bar{X}_n + t_{\alpha/2} S_n / \sqrt{n}] 
\end{align*}

### Unknown distribution
* Use the CLT: $\bar{X}_n \sim \cN(\mu, \sigma^2/n) \rightarrow \cN(0, 1)$
* Also use the LLN: $S_n^2 \rightarrow \sigma^2$
* The rest of the procedure is the same as above

